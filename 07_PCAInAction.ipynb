{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 7: PCA in Action\n",
    "\n",
    "## Goals:\n",
    "- Build a 'PCA' class\n",
    "- Use PCA to remove noise\n",
    "- Compare *many* projections (via scikit-learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Using the data set in `data/UN_IRE_data_smaller.csv` perform PCA. Build your own class to interact with the data.\n",
    "1. Write functions like `__init__` and `__repr__`. \n",
    "2. Write a method to compute all of the principal components (return them in order).\n",
    "3. Find a reasonable $k$ such that almost all of the total variability is captured in the first $k$ principal components. \n",
    "4. Project the data onto the first $k$ components.\n",
    "5. Plot the projected data on the first two components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try it yourself first!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyPCA:\n",
    "\tdef __init__(self, data):\n",
    "\t\tassert isinstance(data, pd.DataFrame)\n",
    "\t\tself.data = data.to_numpy().T\n",
    "\t\tself.feature_names = data.columns.tolist()\n",
    "\t\tself.n = self.data.shape[1]\n",
    "\t\t\n",
    "\t\t# Compute covariance matrix\n",
    "\t\tself.cov_matrix = self.data @ self.data.T / self.n\n",
    "\t\t\n",
    "\t\t# Compute eigenvalues and eigenvectors\n",
    "\t\tevals, evecs = np.linalg.eig(self.cov_matrix)\n",
    "\t\t\n",
    "\t\t# Sort by eigenvalues in descending order\n",
    "\t\tidx = np.argsort(evals)[::-1]\n",
    "\t\tself.eigenvalues = evals[idx]\n",
    "\t\tself.eigenvectors = evecs[:, idx].T\t\t# As rows\n",
    "\t\t\n",
    "\tdef __repr__(self):\n",
    "\t\treturn f\"PCA(n_samples={self.n}, n_features={self.data.shape[0]})\"\n",
    "\t\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.n\n",
    "\t\n",
    "\tdef get_principal_components(self):\n",
    "\t\treturn self.eigenvectors\n",
    "\t\n",
    "\tdef explained_variance_ratio(self):\n",
    "\t\treturn self.eigenvalues / np.sum(self.eigenvalues)\n",
    "\t\n",
    "\tdef cumulative_variance(self):\n",
    "\t\treturn np.cumsum(self.explained_variance_ratio())\n",
    "\t\n",
    "\tdef find_k(self, threshold=0.95):\n",
    "\t\tcum_var = self.cumulative_variance()\n",
    "\t\tk = np.argmax(cum_var >= threshold) + 1\n",
    "\t\treturn k\n",
    "\t\n",
    "\tdef project(self, k):\n",
    "\t\tQ = self.eigenvectors[:k, :]\n",
    "\t\treturn Q @ self.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/UN_IRE_data_smaller.csv')\n",
    "pca = MyPCA(df)\n",
    "print(pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "projected_data = pca.project(2)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(projected_data[0, :], projected_data[1, :])\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio()[0]:.2%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio()[1]:.2%} variance)')\n",
    "plt.title('Data projected onto first two principal components')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is that by removing the principal components with very small eigenvalues, we will remove much of the noise in an image. \n",
    "\n",
    "More specifically:\n",
    "- We will use an (uncorrupted) image dataset and then add noise to it. \n",
    "- We will use PCA to project to a smaller subspace, hopefully removing the noise.\n",
    "\n",
    "\n",
    "Some of the following code in this section comes from a [tutorial](https://scikit-learn.org/stable/auto_examples/applications/plot_digits_denoising.html#sphx-glr-auto-examples-applications-plot-digits-denoising-py) hosted on [scikit-learn](https://scikit-learn.org/stable/index.html):\n",
    "- Authors: The scikit-learn developers\n",
    "- License: BSD-3-Clause\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's describe our dataset. This is a famous dataset of digits coming from the US Mail (US Postal Service). \n",
    "\n",
    "Information about the data set can be found in the:\n",
    "- [research article](https://doi.org/10.1109/34.291440)\n",
    "- [OpenML website](https://www.openml.org/search?type=data&status=active&id=41082)\n",
    "\n",
    "It contains a total of 9298 samples of handwritten digits (zero through nine) as 16 x 16 grayscale images.\n",
    "\n",
    "In other words, we have 9298 points in a 256-dimensional space.\n",
    "\n",
    "A quote from OpenML:\n",
    "\n",
    "> The test set is notoriously \"difficult\", and a 2.5% error rate is excellent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\t# Typical alt to StandardScaler\n",
    "\n",
    "X, y = fetch_openml(data_id=41082, as_frame=False, return_X_y=True)\n",
    "X = MinMaxScaler().fit_transform(X)\t\t\t\t# Scale data\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A helper function to plot the digits for humans to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small helper function to plot 64 digits in 8 x 8 grid\n",
    "def plot_digits(X, title, pixel=16):\n",
    "    fig, axs = plt.subplots(nrows=10, ncols=10, figsize=(6, 6))\n",
    "    for img, ax in zip(X, axs.ravel()):\n",
    "        ax.imshow(img.reshape((pixel, pixel)), cmap=\"Greys\")\n",
    "        ax.axis(\"off\")\n",
    "    fig.suptitle(title, fontsize=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's plot a sample of 64 digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X, \"A Sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is common practice in machine learning to only test on a random sample of the data.\n",
    "\n",
    "This way, one can \"test\" the findings on the unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, random_state=0, train_size=1_000, test_size=100\n",
    ")\n",
    "\n",
    "# We add Gaussian noise to the images\n",
    "rng = np.random.RandomState(0)\n",
    "noise = rng.normal(scale=0.25, size=X_test.shape)\n",
    "X_test_noisy = X_test + noise\n",
    "\n",
    "noise = rng.normal(scale=0.25, size=X_train.shape)\n",
    "X_train_noisy = X_train + noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X_test, \"Uncorrupted test images\")\n",
    "plot_digits(\n",
    "    X_test_noisy, f\"Noisy test images\\n(MSE: {np.mean((X_test - X_test_noisy) ** 2):.2f})\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply PCA. We will use the one in scikit-learn for formatting convenience.\n",
    "\n",
    "We are also going to use an *extension* of PCA called [Kernel PCA](https://en.wikipedia.org/wiki/Kernel_principal_component_analysis).\n",
    "\n",
    "I am bringing in KPCA simply for exposure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, KernelPCA\n",
    "\n",
    "pca = PCA(n_components=32, random_state=42)\n",
    "kernel_pca = KernelPCA(\n",
    "    n_components=400,\n",
    "    kernel=\"rbf\",\n",
    "    gamma=1e-3,\n",
    "    fit_inverse_transform=True,\n",
    "    alpha=5e-3,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "pca.fit(X_train_noisy)\n",
    "_ = kernel_pca.fit(X_train_noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project (but keep it in original space so we can make pictures)\n",
    "X_reconstructed_kernel_pca = kernel_pca.inverse_transform(\n",
    "    kernel_pca.transform(X_test_noisy)\n",
    ")\n",
    "X_reconstructed_pca = pca.inverse_transform(pca.transform(X_test_noisy))\n",
    "\n",
    "# Now we plot the pictures\n",
    "plot_digits(X_test, \"Uncorrupted test images\")\n",
    "plot_digits(\n",
    "    X_reconstructed_pca,\n",
    "    f\"PCA reconstruction\\nMSE: {np.mean((X_test - X_reconstructed_pca) ** 2):.2f}\",\n",
    ")\n",
    "plot_digits(\n",
    "    X_reconstructed_kernel_pca,\n",
    "    (\n",
    "        \"Kernel PCA reconstruction\\n\"\n",
    "        f\"MSE: {np.mean((X_test - X_reconstructed_kernel_pca) ** 2):.2f}\"\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will take a look at many different projections. Recall, we have discussed two different kinds: \n",
    "1. PCA \n",
    "2. random projections (via JL).\n",
    "\n",
    "We use another [tutorial](https://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html#sphx-glr-auto-examples-manifold-plot-lle-digits-py) from scikit-learn, nearly verbatim:\n",
    "- Authors: The scikit-learn developers\n",
    "- License: BSD-3-Clause\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a simplified digits dataset:\n",
    "- only first 6 digits\n",
    "- 8 x 8 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits(n_class=6)\n",
    "X, y = digits.data, digits.target\n",
    "n_samples, n_features = X.shape\n",
    "n_neighbors = 30\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_digits(X, \"A Sample\", pixel=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set up our plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import offsetbox\n",
    "\n",
    "def plot_embedding(X, title, with_digit=True):\n",
    "    _, ax = plt.subplots()\n",
    "    X = MinMaxScaler().fit_transform(X)\n",
    "\n",
    "    for digit in digits.target_names:\n",
    "        ax.scatter(\n",
    "            *X[y == digit].T,\n",
    "            marker=f\"${digit}$\",\n",
    "            s=60,\n",
    "            color=plt.cm.Dark2(digit),\n",
    "            alpha=0.425,\n",
    "            zorder=2,\n",
    "        )\n",
    "    shown_images = np.array([[1.0, 1.0]])  # just something big\n",
    "    if with_digit:\n",
    "        for i in range(X.shape[0]):\n",
    "            # plot every digit on the embedding\n",
    "            # show an annotation box for a group of digits\n",
    "            dist = np.sum((X[i] - shown_images) ** 2, 1)\n",
    "            if np.min(dist) < 4e-3:\n",
    "                # don't show points that are too close\n",
    "                continue\n",
    "            shown_images = np.concatenate([shown_images, [X[i]]], axis=0)\n",
    "            imagebox = offsetbox.AnnotationBbox(\n",
    "                offsetbox.OffsetImage(digits.images[i], cmap=plt.cm.gray_r), X[i]\n",
    "            )\n",
    "            imagebox.set(zorder=1)\n",
    "            ax.add_artist(imagebox)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we harvest all of the comparisons we want to do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.manifold import (\n",
    "    MDS,\n",
    "    TSNE,\n",
    "    Isomap,\n",
    "    LocallyLinearEmbedding,\n",
    "    SpectralEmbedding,\n",
    ")\n",
    "from sklearn.neighbors import NeighborhoodComponentsAnalysis\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.random_projection import SparseRandomProjection\n",
    "\n",
    "embeddings = {\n",
    "\t\"Principal Component Analysis\": PCA(n_components=2),\n",
    "    \"Random projection embedding\": SparseRandomProjection(\n",
    "        n_components=2, random_state=42\n",
    "    ),\n",
    "    \"Truncated SVD embedding\": TruncatedSVD(n_components=2),\n",
    "    \"Linear Discriminant Analysis embedding\": LinearDiscriminantAnalysis(\n",
    "        n_components=2\n",
    "    ),\n",
    "    \"Isomap embedding\": Isomap(n_neighbors=n_neighbors, n_components=2),\n",
    "    \"Standard LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"standard\"\n",
    "    ),\n",
    "    \"Modified LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"modified\"\n",
    "    ),\n",
    "    \"Hessian LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"hessian\"\n",
    "    ),\n",
    "    \"LTSA LLE embedding\": LocallyLinearEmbedding(\n",
    "        n_neighbors=n_neighbors, n_components=2, method=\"ltsa\"\n",
    "    ),\n",
    "    \"MDS embedding\": MDS(n_components=2, n_init=1, max_iter=120, eps=1e-6),\n",
    "    \"Random Trees embedding\": make_pipeline(\n",
    "        RandomTreesEmbedding(n_estimators=200, max_depth=5, random_state=0),\n",
    "        TruncatedSVD(n_components=2),\n",
    "    ),\n",
    "    \"Spectral embedding\": SpectralEmbedding(\n",
    "        n_components=2, random_state=0, eigen_solver=\"arpack\"\n",
    "    ),\n",
    "    \"t-SNE embedding\": TSNE(\n",
    "        n_components=2,\n",
    "        max_iter=500,\n",
    "        n_iter_without_progress=150,\n",
    "        n_jobs=2,\n",
    "        random_state=0,\n",
    "    ),\n",
    "    \"NCA embedding\": NeighborhoodComponentsAnalysis(\n",
    "        n_components=2, init=\"pca\", random_state=0\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "\n",
    "projections, timing = {}, {}\n",
    "for name, transformer in embeddings.items():\n",
    "    if name.startswith(\"Linear Discriminant Analysis\"):\n",
    "        data = X.copy()\n",
    "        data.flat[:: X.shape[1] + 1] += 0.01  # Make X invertible\n",
    "    else:\n",
    "        data = X\n",
    "\n",
    "    print(f\"Computing {name}...\")\n",
    "    start_time = time()\n",
    "    projections[name] = transformer.fit_transform(data, y)\n",
    "    timing[name] = time() - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in timing:\n",
    "    title = f\"{name} (time {timing[name]:.3f}s)\"\n",
    "    plot_embedding(projections[name], title, with_digit=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can read summaries of most of these dimension-reduction methods here:\n",
    "\n",
    "[Nonlinear dimension reduction](https://en.wikipedia.org/wiki/Nonlinear_dimensionality_reduction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
