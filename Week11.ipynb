{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 11: Hierarchical clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals \n",
    "- Examples of hierarchical clustering\n",
    "- Perform hierarchical clustering\n",
    "- Compare to $k$-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wine\n",
    "\n",
    "Let's take a look at SLINK and CLINK using a wine data set.\n",
    "\n",
    "Data set was obtained from [kaggle.com](https://www.kaggle.com/datasets/harrywang/wine-dataset-for-clustering/). The original [data set](https://archive.ics.uci.edu/dataset/109/wine) is publicly available with the [CC-BY 4.0](https://creativecommons.org/licenses/by/4.0/legalcode) license. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/wine-clustering.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 13 columns, so our data points live in $\\mathbb{R}^{13}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly run a PCA and project the data onto the first two principal components. \n",
    "\n",
    "Here is code to scale the data, but we could probably avoid it. We definitely need to translate the data, so that the mean is at the origin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mat_to_rescaled_mat(Z):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(Z)\n",
    "    return scaler.transform(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll work with two data sets: `X` and `Z`.\n",
    "\n",
    "- `X` : raw data \n",
    "- `Z` : scaled data (via StandardScaler in `sklearn`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From `scikit-learn`'s [website](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html):\n",
    "\n",
    "> Standardization of a dataset is a common requirement for many \n",
    "> machine learning estimators: they might behave badly if the \n",
    "> individual features do not more or less look like standard \n",
    "> normally distributed data (e.g. Gaussian with 0 mean and \n",
    "> unit variance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.to_numpy()\n",
    "Z = mat_to_rescaled_mat(X)\n",
    "print(\"First 3 rows of X:\")\n",
    "print(X[:3,:])\n",
    "print(\"\\nFirst 3 rows of Z:\")\n",
    "print(Z[:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(Z)\n",
    "Y = pca.transform(Z)\n",
    "plt.figure(1)\n",
    "_ = plt.scatter(Y[:,0], Y[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also briefly look at a $k$-means clustering. \n",
    "\n",
    "First let's see if we can get any information from an elbow plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MyKMeans(k, data):\n",
    "    return KMeans(n_clusters=k, n_init=\"auto\").fit(data)\n",
    "\n",
    "def ElbowMethodPlot(data, N):\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    def WCSS(data, kmeans):\n",
    "        from functools import reduce\n",
    "        ord_cents = map(lambda i: kmeans.cluster_centers_[i], kmeans.labels_)\n",
    "        tups = zip(data, ord_cents)\n",
    "        dist2 = lambda t: np.linalg.norm(t[0] - t[1])**2\n",
    "        return reduce(lambda x, y: x + dist2(y), tups, 0)\n",
    "\n",
    "    KMEANS = [MyKMeans(k, data) for k in range(1, N + 1)]\n",
    "    WCSS_vals = [WCSS(data, kmeans) for kmeans in KMEANS]\n",
    "\n",
    "    plt.figure(1)\n",
    "    plt.grid()\n",
    "    plt.plot(WCSS_vals)\n",
    "    plt.xticks(ticks=range(N), labels=range(1, N+1))\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = ElbowMethodPlot(Z, 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_data = MyKMeans(3, Z)\n",
    "plt.clf()\n",
    "_ = plt.scatter(Y[:,0], Y[:,1], c=kmeans_data.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will do this with a hierarchical clustering algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `scikit-learn` to do hierarchical clustering, but `scipy` has a very good function as well. Moreover, `scipy` has a `dendrogram` function, which is built around the out of their own clustering algorithm.\n",
    "\n",
    "Instead, we will just use the following dendrogram plotting function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dendrogram(model, **kwargs):\n",
    "    import numpy as np\n",
    "    # from matplotlib import pyplot as plt\n",
    "    from scipy.cluster.hierarchy import dendrogram\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SLINK(data):\n",
    "    return AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage=\"single\").fit(data)\n",
    "\n",
    "def CLINK(data):\n",
    "    return AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage=\"complete\").fit(data)\n",
    "\n",
    "def ALINK(data):\n",
    "    return AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage=\"average\").fit(data)\n",
    "\n",
    "def WLINK(data):\n",
    "    return AgglomerativeClustering(n_clusters=None, distance_threshold=0, linkage=\"ward\").fit(data)\n",
    "\n",
    "def HCluster(k, data, link=\"ward\"):\n",
    "    return AgglomerativeClustering(n_clusters=k, linkage=link).fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dendrogram(WLINK(X), truncate_mode=\"level\", p=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how hierarchical clustering compares with out $k$-means clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hclus_data = HCluster(3, Z, link=\"ward\")\n",
    "plt.clf()\n",
    "_ = plt.scatter(Y[:,0], Y[:,1], c=hclus_data.labels_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was at this stage that I dug through the original data for more information about the wine.\n",
    "\n",
    "I found the following quote:\n",
    "\n",
    "> These data are the results of a chemical analysis of wines grown in the same region in Italy but derived from three different cultivars.\n",
    "> \n",
    "> The analysis determined the quantities of 13 constituents found in each of the three types of wines. \n",
    "\n",
    "ðŸ¤¯ðŸ¤¯ðŸ¤¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ward-linkage\n",
    "\n",
    "Recall we have given formulae for the \"single\", \"complete\" and \"average\" linkages.\n",
    "\n",
    "If $C_1$ and $C_2$ are finite (disjoint) subsets of $\\mathbb{R}^m$, then \n",
    "1. `SINGLE` is given as\n",
    "$$\n",
    "    d(C_1, C_2) = \\min\\left\\{ \\| x - y\\| : x\\in C_1, y\\in C_2 \\right\\},\n",
    "$$\n",
    "2. `COMPLETE` is given as \n",
    "$$\n",
    "    d(C_1, C_2) = \\max\\left\\{ \\| x - y\\| : x\\in C_1, y\\in C_2 \\right\\},\n",
    "$$\n",
    "3. `AVERAGE` is given as \n",
    "$$\n",
    "    d(C_1, C_2) = \\sum_{x \\in C_1} \\sum_{y\\in C_2} \\dfrac{\\|x - y\\|}{\\# C_1 \\cdot \\# C_2},\n",
    "$$\n",
    "4. `WARD` is given as \n",
    "$$\n",
    "    d(C_1, C_2) = \\sum_{x\\in C_1}\\sum_{y\\in C_2} \\dfrac{\\|x - y\\|^2}{\\# C_1 + \\# C_2} - \\sum_{x\\in C_1}\\sum_{y\\in C_1} \\dfrac{\\|x - y\\|^2}{\\# C_1} - \\sum_{x\\in C_2}\\sum_{y\\in C_2} \\dfrac{\\|x - y\\|^2}{\\# C_2}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind the `WARD` linkage is to join clusters $C_1$ and $C_2$ only if they are the pair that minimizes the WCSS (Within-Cluster Sum of Squares).\n",
    "\n",
    "We will use `WARD` here in this notebook, but you do not need to commit it to memory. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples with hierarchical clustering\n",
    "\n",
    "Let's go through the same data sets we went through with $k$-means and see how hierarchical compares.\n",
    "\n",
    "We will use `HCluster` defined above, and for plotting let's use the following code. Similar format as `Voronoi` from Week10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hcluster(data, hcluster=None):\n",
    "    import matplotlib.cm as cm\n",
    "    import matplotlib.pyplot as plt \n",
    "    plt.figure(1)\n",
    "    plt.clf()\n",
    "    if hcluster:\n",
    "        plt.scatter(data[:,0], data[:,1], c=hcluster.labels_)\n",
    "    else:\n",
    "        plt.scatter(data[:,0], data[:,1], c=\"black\")\n",
    "    return plt "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three clear clusters\n",
    "\n",
    "First up is `data/three_clusters_clear.csv`. Just to remind us what it looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3c = pd.read_csv(\"data/three_clusters_clear.csv\").to_numpy()\n",
    "plot = plot_hcluster(data_3c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcluster_3c = HCluster(3, data_3c, link=\"single\")\n",
    "plot = plot_hcluster(data_3c, hcluster=hcluster_3c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does the corresponding dendrogram look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = plot_dendrogram(SLINK(data_3c), truncate_mode=\"level\", p=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the three clusters:\n",
    "- there are 58 points in orange: 54 points plus points labeled 0, 150, 81, and 114.\n",
    "- there are two sets of 58 points in green.\n",
    "  - left: 55 points plus 2 points plus a point labeled 104.\n",
    "  - right: 56 points plus points labeled 166 and 16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Three vague clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_3v = pd.read_csv(\"data/three_clusters_vague.csv\").to_numpy()\n",
    "plot = plot_hcluster(data_3v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcluster_3v = HCluster(3, data_3v, link=\"single\")\n",
    "plot = plot_hcluster(data_3v, hcluster=hcluster_3v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two unbalanced clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It was only now that I realised I had a typo in my `csv` file..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_2u = pd.read_csv(\"data/two_clusters_unbalanced.csv\").to_numpy()\n",
    "plot = plot_hcluster(data_2u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcluster_2u = HCluster(2, data_2u, link=\"single\")\n",
    "plot = plot_hcluster(data_2u, hcluster=hcluster_2u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The 'impossible' data set\n",
    "\n",
    "We can see hierarchical clustering shine compared to $k$-means with this 'impossible' data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_i = pd.read_csv(\"data/impossible.csv\").to_numpy()\n",
    "plot = plot_hcluster(data_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of clusters is not so clear-cut as the previous examples. Let's try a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcluster_i = HCluster(2, data_i, link=\"single\")\n",
    "plot = plot_hcluster(data_i, hcluster=hcluster_i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it yourself!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are five data sets to play around with. Feel free to use the functions here: \n",
    "- `plot_dendrogram` with:\n",
    "  - `SLICK`, `CLINK`, `ALINK`, `WLINK`,\n",
    "- `HCluster`, \n",
    "- `plot_hcluster`.\n",
    "\n",
    "Here are some question to consider.\n",
    "1. How many clusters are there in the data? \n",
    "2. Do the clusters seem to be unambiguous? \n",
    "3. What other intrepretations are there?\n",
    "4. Does the hierarchical clustering do what you expect? Why? \n",
    "5. Does one linkage perform far better (or far worse) than the others? Why do you think?\n",
    "\n",
    "Discuss this with a neighbour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data sets:\n",
    "- `extra_1.csv`\n",
    "- `extra_2.csv`\n",
    "- `extra_3.csv`\n",
    "- `extra_4.csv`\n",
    "- `extra_5.csv`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
